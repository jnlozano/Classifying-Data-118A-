{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_KNeighborsClassifier(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "    def predict(self, X_pred):\n",
    "        Y_pred = []\n",
    "        length = len(X_pred)\n",
    "        distances = np.zeros(len(self.X_train))\n",
    "\n",
    "        for i in range (length):\n",
    "            for difference in range (len(self.X_train)):\n",
    "                distances[difference] = np.linalg.norm(X_pred[i]-self.X_train[difference])\n",
    "                \n",
    "            min = np.argpartition(distances, self.k)\n",
    "            \n",
    "            labels = []\n",
    "            for x in range (self.k):\n",
    "                index = min[x]\n",
    "                label = int(self.Y_train[index])\n",
    "                labels.append(label)\n",
    "                \n",
    "                \n",
    "            half = len(labels)/2\n",
    "            num_ones = np.count_nonzero(labels)\n",
    "            if num_ones > half:\n",
    "                prediction = 1\n",
    "            if num_ones < half:\n",
    "                prediction = 0\n",
    "            if num_ones == half:\n",
    "                prediction = 1\n",
    "            \n",
    "            Y_pred.append(prediction)\n",
    "\n",
    "        \n",
    "        return np.array(Y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def Report(classifier, xTest, yTest):\n",
    "    \n",
    "    tme = time.time()\n",
    "    yPred = classifier.predict(xTest)\n",
    "    elapsed_time = time.time() - tme\n",
    "    \n",
    "    print(\"Model prediction time (s): \"+ str(elapsed_time))\n",
    "    \n",
    "    print (\"Model accuracy: \" + str(classifier.score(xTest, yTest)))\n",
    "    \n",
    "    print(\"f1 accuracy: \" + str(f1_score(yTest,yPred)))\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print('\\n')\n",
    "    print(classification_report(yTest, yPred, digits=4))\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "def draw_heatmap_linear(acc, acc_desc, C_list):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(acc, annot=True, fmt='.3f', yticklabels=C_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label(\"accuracy\")\n",
    "    ax.set(ylabel='$C$')\n",
    "    plt.title(acc_desc + ' w.r.t $C$')\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    plt.show()\n",
    "    \n",
    "def simple_cross_validation(X_train_val, Y_train_val, k, fold):\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    \n",
    "    length = X_train_val.shape[0]\n",
    "    subset_size = length/fold\n",
    " \n",
    "    for i in range(fold):\n",
    "        all_data = np.arange(length)\n",
    "        validation_set = np.arange(int(np.round(i*subset_size)),int(np.round((i+1)*subset_size)))\n",
    "        training_set = np.setdiff1d(all_data,validation_set)\n",
    "        \n",
    "        train_X = X_train_val[training_set,:]\n",
    "        validation_X = X_train_val[validation_set,:]\n",
    "        train_Y = Y_train_val[training_set]\n",
    "        validation_Y = Y_train_val[validation_set]\n",
    "        \n",
    "        classifier = simple_KNeighborsClassifier(k)\n",
    "        classifier.fit(train_X,train_Y)\n",
    "        train_pred = classifier.predict(train_X)\n",
    "        val_pred = classifier.predict(validation_X)\n",
    "        \n",
    "        train_acc = np.count_nonzero((train_pred-train_Y)==0)/(train_Y.shape[0])\n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc = np.count_nonzero((val_pred-validation_Y)==0)/(validation_Y.shape[0]) \n",
    "        val_acc_list.append(val_acc)\n",
    "        \n",
    "        \n",
    "    return sum(val_acc_list) / len(val_acc_list), \\\n",
    "           sum(train_acc_list) / len(train_acc_list)\n",
    "    \n",
    "def simple_GridSearchCV_fit(X_train_val, Y_train_val, k_list, fold):\n",
    "    val_acc_array = np.zeros(len(k_list))\n",
    "    train_acc_array = np.zeros(len(k_list))\n",
    "    for i in range(len(k_list)):\n",
    "        val_acc_array[i], train_acc_array[i] = simple_cross_validation(\n",
    "            X_train_val, Y_train_val, k_list[i], fold)\n",
    "    return val_acc_array, train_acc_array\n",
    "        \n",
    "def draw_heatmap_knn(acc, acc_desc, k_list):\n",
    "    plt.figure(figsize = (2,4))\n",
    "    ax = sns.heatmap(acc, annot=True, fmt='.3f', yticklabels=k_list, xticklabels=[])\n",
    "    ax.collections[0].colorbar.set_label(\"accuracy\")\n",
    "    ax.set(ylabel='$k$')\n",
    "    plt.title(acc_desc + ' w.r.t $k$')\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_CORES = 32\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm as svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "import time\n",
    "    \n",
    "def KNN(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"KNN:\")\n",
    "    clf = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors' : [x for x in np.arange(1,24) if x%2 == 1] }\n",
    "    clas = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=CPU_CORES)\n",
    "    t = time.time()\n",
    "    clas.fit(xTrain, yTrain)\n",
    "    timeEnd = time.time() - t\n",
    "    print(\"Best K: \" +str(clas.best_params_['n_neighbors']))\n",
    "    print(\"Training time (s):\" + str(timeEnd))\n",
    "    Report(clas, xTest, yTest)\n",
    "\n",
    "def RandomForest(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"Random Forest:\")\n",
    "    clf = RandomForestClassifier(n_estimators = 200)\n",
    "    param_grid = {\n",
    "        'max_features': [x for x in [1,2,4,6,8,12,16,20] if x < len(xTrain[0])]\n",
    "    }\n",
    "    clas = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=CPU_CORES)\n",
    "    tme = time.time()\n",
    "    clas.fit(xTrain, yTrain)\n",
    "    timeEnd = time.time() - tme\n",
    "    print(\"Best MaxFeatures: \" + str(clas.best_params_['max_features']))\n",
    "    print(\"Training Time (s): \" + str(timeEnd))\n",
    "    Report(clas, xTest, yTest)\n",
    "    \n",
    "def SVMlin(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"SVM w/Linear kernel:\")\n",
    "    svc = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, gamma='auto', kernel='linear',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "\n",
    "    param_grid = {'C': [0.001, 0.01,0.1, 1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "    clf = GridSearchCV(svc, param_grid=param_grid, cv=5, n_jobs = CPU_CORES)\n",
    "    tme = time.time()\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    timeEnd = time.time() - tme\n",
    "    print(\"Best C: \"  + str(clf.best_params_['C']))\n",
    "    print(\"Training time (s):\" + str(timeEnd))\n",
    "    Report(clf, xTest, yTest)\n",
    "    \n",
    "def SVMrbf(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"SVM w/ RBF Kernel:\")\n",
    "\n",
    "    svc = LinearSVC(C=1.0, class_weight=None,\n",
    "        max_iter=-1, random_state=None,\n",
    "        tol=0.001, verbose=False)\n",
    "    classifier = svm.SVC(kernel='rbf')\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001,0.01,0.1, 1, 2, 3, 4, 5, 6], 'gamma': [500000, 20000, 5000, 200, 50, 2, 0.5, 0.125, 0.01]\n",
    "    }\n",
    "\n",
    "    CV = GridSearchCV(classifier, param_grid=param_grid, cv=5, n_jobs = CPU_CORES)\n",
    "    tme = time.time()\n",
    "    CV.fit(xTrain, yTrain)\n",
    "    timeEnd = time.time() - tme\n",
    "    print(\"Best C: \" + str(CV.best_params_['C']))\n",
    "    print(\"Best gamma: \" + str(CV.best_params_['gamma']))\n",
    "    print(\"Training time (s):\" + str(timeEnd))\n",
    "    Report(CV, xTest, yTest)\n",
    "    \n",
    "def BoostedDecisionTree(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"Decision Tree:\")\n",
    "    param_grid = {'n_estimators': [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]}\n",
    "    clf = AdaBoostClassifier()\n",
    "    CV = GridSearchCV(estimator=clf, param_grid = param_grid, cv = 5, n_jobs = CPU_CORES)\n",
    "    t = time.time()\n",
    "    CV.fit(xTrain, yTrain)\n",
    "    elapsed_time = time.time() - t\n",
    "    print(\"Best nEstimators: \" + str(CV.best_params_['n_estimators']))\n",
    "    print(\"Training time (s):\"+ str(elapsed_time))\n",
    "    Report(CV, xTest, yTest)\n",
    "\n",
    "def XGBoost(xTrain, xTest, yTrain, yTest):\n",
    "    xTrain = np.array(xTrain);\n",
    "    xTest = np.array(xTest);\n",
    "    yTrain = np.array(yTrain);\n",
    "    yTest = np.array(yTest);\n",
    "    \n",
    "    print(\"XGBoost:\")\n",
    "    param_grid = {n_estimators': [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]}\n",
    "        \n",
    "    clf = xgb.XGBClassifier()\n",
    "    CV = GridSearchCV(estimator=clf, param_grid = param_grid, cv = 5)\n",
    "    t = time.time()\n",
    "    CV.fit(xTrain, yTrain)\n",
    "    elapsed_time = time.time() - t\n",
    "    print(\"Best nEstimators: \" + str(CV.best_params_['n_estimators'])) \n",
    "    print(\"Training time (s):\" + str(elapsed_time))\n",
    "    Report(CV, xTest, yTest)\n",
    "    \n",
    "    \n",
    "def NeuralNets(xTrain, xTest, yTrain, yTest):\n",
    "    print(\"Neural Nets(ANN):\")\n",
    "    param_grid = {\n",
    "        'learning_rate_init': [10e-4, 10e-3, 10e-2, 10e-1]\n",
    "    }\n",
    "    clf = MLPClassifier(hidden_layer_sizes = (640,), solver ='sgd', early_stopping = True, nesterovs_momentum = False, learning_rate = 'constant')\n",
    "    CV = GridSearchCV(estimator=clf, param_grid = param_grid, cv = 5)\n",
    "    t = time.time()\n",
    "    CV.fit(xTrain, yTrain)\n",
    "    elapsed_time = time.time() - t\n",
    "    print(\"Best LearningRate: \" + \"{0:.3f}\".format(CV.best_params_['learning_rate_init']))\n",
    "    print(\"Training time(s):\"+ str(elapsed_time))\n",
    "    Report(CV, xTest, yTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
